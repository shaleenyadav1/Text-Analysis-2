# Function to group documents in a corpus by similarity using a combination of text 
# and graph based analytics
install.packages("slam")
install.packages("tm")
install.packages("SnowballC")
install.packages("RColorBrewer")
install.packages("plotrix")
install.packages("plyr")
install.packages("ggplot2")
install.packages("tau")

library(ggplot2)
require(tm)
library(SnowballC) 
library(igraph)
library(lsa)
library(wordcloud)
library(qdap)
require(wordcloud)
require(plotrix)
library(tau)

#options(warn=1)
# Set Cosine Similarity threshold
cosSimThres<-.9
# Set the LSA Rank
lsaRank<-25

# setting directory

setwd("C://Users//Rakhi//Desktop//rakesh//course//BI with R//project")
getwd
# Process the corpus, might be duplicate ticket entries so take the very first incidence of each ticket
debatedata<-read.csv(file="debate.csv")
# Filtering Clinton speech
view(debatedata)
Clintonx<-debatedata[debatedata$Speaker == "Clinton", 3]
Trumpx<-debatedata[debatedata$Speaker == "Trump", 3]
Csource<-VectorSource(Clintonx)
Tsource<-VectorSource(Trumpx)
Ccorpus<-Corpus(Csource)
Tcorpus<-Corpus(Tsource)
print(Ccorpus)
names(Csource)
englishwords<-read.csv("C://Users//Rakhi//Desktop//rakesh//course//BI with R//project//english.csv")
english<-as.vector(englishwords[,1])
print(english)
# creating a function for Pre-cleaning steps like stemming, remomving stopwords for the clinton and trump speech data
clean_data <- function(cleandata){
  dictCorpus<-cleandata
  cleandata <- tm_map(cleandata, removePunctuation)
  cleandata <- tm_map(cleandata, stripWhitespace)
  cleandata <- tm_map(cleandata, removeNumbers)
  cleandata <- tm_map(cleandata, content_transformer(tolower))
  #cleandata <- tm_map(cleandata, stemDocument)
  cleandata <- tm_map(cleandata, removeWords,english )
  #cleandata <- tm_map(cleandata, stemCompletion, dictionary=dictCorpus)
  #cleandata <- tm_map(cleandata, content_transformer(stemCompletion), dictionary = dictCorpus, lazy=TRUE)
  #cleandata <- tm_map(cleandata,PlainTextDocument)
return(cleandata)
}

clinton_cleaned <- clean_data(Ccorpus)
trump_cleaned <- clean_data(Tcorpus)
inspect(clinton_cleaned[1:5])
print(clinton_cleaned[1:5])
#print(Csource)
Clinton_Dmatrix <- TermDocumentMatrix(clinton_cleaned, control = list(minWordLength = 1))
Trump_Dmatrix <- TermDocumentMatrix(trump_cleaned, control = list(minWordLength = 1))
inspect(Clinton_Dmatrix)
#inspect(myDtm[266:270,31:40])

findFreqTerms(Trump_Dmatrix)
#library(wordcloud)
m <- as.matrix(Clinton_Dmatrix)
 # calculate the frequency of words
#print(m)
v <- sort(rowSums(m), decreasing=TRUE)
print(v)
myNames <- names(v)
print(myNames)
#gives the matrix position
k <- which(names(v)=="child")
d <- data.frame(word=myNames, freq=v)
print(d)
#png("wordcloud_packages.png", width=12,height=8, units='in', res=300)
wordcloud(d$word, d$freq,min.freq=3, colors= brewer.pal(5,"Set1"))


# trump word cloud
n <- as.matrix(Trump_Dmatrix)
w <- sort(rowSums(n), decreasing=TRUE)
print(w)
myNames_T <- names(w)
print(myNames_T)
#gives the matrix position
#k <- which(names(v)=="child")
c <- data.frame(word=myNames_T, freq=w)
print(c)
#png("wordcloud_packages.png", width=12,height=8, units='in', res=300)
wordcloud(c$word, c$freq,min.freq=20, colors= brewer.pal(5,"Set1"))

ctemp<-findFreqTerms(Clinton_Dmatrix,min,max)
print(Clinton_Dmatrix)
top<-data.frame(sort(rowSums(m), decreasing=TRUE)[1:10])
print(top)
names(top)[]
class(top)
ggplot(data=top)



tokenize_ngrams <- function(x, n=2) {  return(textcnt(x,method="string",n=n,decreasing=TRUE))}

bigram_vit <- function(corpus){
  sample_df <- data.frame(text=unlist(sapply(corpus, '[',"content")),stringsAsFactors=F)
  bigrams_df <- tokenize_ngrams(sample_df,n=2) 
  # bigrams_df <- sample_df %>% tokenize_ngrams(n=2) 
  bigrams_df <- data.frame(word = rownames(as.data.frame(unclass(bigrams_df))),
                           freq = unclass(bigrams_df))
  return(bigrams_df)
}

#For Clinton
corpus <- clinton_cleaned
bigram_clin <- bigram_vit(corpus)

#For Trump
corpus <- trump_cleaned
bigram_trum <- bigram_vit(corpus)

plot_barplot(bigram_clin[1:10,],"Clinton")

colnames(bigram_clin)[2] <-"Clinton"
colnames(bigram_trum)[2] <-"Trump"
all_m <- merge(bigram_clin,bigram_trum,all = T)
all_m[is.na(all_m)]<-0
rownames(all_m) <- all_m$word
all_m <- all_m[,2:3]

#Subset shared terms
common_words <- subset(all_m,all_m[, 1] > 0 & all_m[, 2] > 0)
# Find most commonly shared words
difference <- abs(common_words[, 1] - common_words[, 2])
common_words <- cbind(common_words, difference)
common_words <- common_words[order(common_words[, 3],
                                   decreasing = TRUE), ]
top25_df <- data.frame(x = common_words[1:25, 1],
                       y = common_words[1:25, 2],
                       labels = rownames(common_words[1:25, ]))


pyramid.plot(top25_df$x, top25_df$y,
             labels = top25_df$labels,
             main = "Words in Common",
             gap = 8, laxlab = 0:18,
             raxlab = 0:18, unit = NULL,
             top.labels = c("Clinton",
                            "Words",
                            "Trump"))

plot_barplot <- function(bigram,tit){
  # Create a barplot
  bigram$word <- factor(bigram$word, levels = bigram$word)
  ggplot(bigram,aes(x= reorder(word,freq), y= freq,fill= freq))+
    geom_bar(stat="identity")+coord_flip() + ggtitle(tit)+
    theme(axis.title.y = element_blank())
}
